# Ch2

教師あり学習について

* 入出力のペアが訓練データとなり、モデルを構築する
  * 一定量のデータが必要不可欠
* Classification クラス分類 vs Regression 回帰
* Classification
  * 1. binary classification 2クラス分類
    * Yes/No 問題 -> positive/netagtive という言い方をする
    * e.g.) メールのスパム判定
  * 2. multiclass classification 多クラス分類
    * e.g.) アイリスの花の分類, web サイトのテキストから言語を判定する
* Regression
  * floating-point number 浮動小数点数の予測
  * 出力になんらかの **連続性** があれば回帰を使う
  * e.g.)
    * 年収(の量 amount)を学歴, 年齢, 住所から推定する
    * とうもろこしの収穫量を前年の収穫量、天候、従業員数から予測
* 単純なモデルの方が、新しいデータに対してよく汎化できる
  * 逆に、過度に複雑なモデルを作ってしまうことを **過剰適合 overfitting** という
  * 単純すぎるモデルは **適合不足 underfitting**
  * 複雑さを上げると個々のデータに対しての精度は上がるが、汎用的ではなくなる
     * 精度と汎用性の tradeoff
* モデルをいじりまわすより、データを増やした方がいいケースも多い(直感的にそんな気もする)
* KNeighbors分類機
  * 近傍点の数、データポイント間の距離測度が重要な parameters
  * 実際にはほとんど使われてない
  * メリット
    * モデルの理解のしやすさ
    * あまり調整しなくても高い性能が出ることが多い
    * ベースラインとして利用可能
  * デメリット
    * 訓練セットが多くなると予測が遅くなる
    * 多数(数百以上)の特徴量を持つデータセットではうまく機能しない
    * ほとんどの特徴量が多くの場合0となるようなデータセットでは性能が悪い
* 線形モデル Linear Model
  * 入力特徴量の **線形関数 linear function** を用いて予測を行う
  * target y が特徴量の線形和で表すことができるという強い仮定を置いている
  * 訓練データのデータポイント数よりも特徴量の方が多い場合は、どのような y でも完全に訓練データセットの線形関数としてモデル化できる
  * 線形回帰(通常最小二乗法 ordinary least squares: OLS)
    * 訓練データにおいて、予測と真の回帰ターゲットyとの **平均二乗誤差 mean squared error** が最小になるように、 w, b を求める
    * w: 傾きを表す。係数 coefficient
    * b: オフセット、切片。intercept
    * sklearn では、訓練データから得られた parameters の変数名には `_` suffix がつく。ユーザが設定する parameters と区別するため
    * 訓練データに対し、過剰適合しがち
  * リッジ回帰 Ridge Regression
    * 予測に使う式は OLS と同じ
    * w を訓練データに対する予測だけでなく、他の制約に対しても最適化する(?)
    * 過剰適合の危険が少ない（訓練セットに対する性能は低いが、汎化性能は高い）
    * parameter `alpha` の値を調節して、汎化性能が高いものを探す
      * alpha が大きくなるとモデルがより制約される
    * 訓練セットの数が少ないうちはこっちの方がいい
  * Lasso
    * Ridge 同様、係数が0になるように制約をかける(L1正則化)
    * 特徴量がたくさんあっても使うものが限られると推測できるケースでは Ridge よりこちらがいい
  * L1/L2正則化
    * 一部の特徴量だけが重要なら前者、デフォルトではL2を採用する
  * 線形モデルの訓練・予測は非常に高速
  * 予測手法が比較的理解しやすい
